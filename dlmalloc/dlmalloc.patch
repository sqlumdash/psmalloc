Copyright (c) 2019 Toshiba Corporation

--- dlmalloc/malloc.c	2020-02-19 17:18:59.688398842 +0900
+++ psm_dlmalloc.c	2020-07-01 08:34:05.045528328 +0900
@@ -520,6 +520,7 @@ MAX_RELEASE_CHECK_RATE   default: 4095 u
   disable, set to MAX_SIZE_T. This may lead to a very slight speed
   improvement at the expense of carrying around more memory.
 */
+#include "psm_dlmalloc.h"
 
 /* Version identifier to allow people to support multiple versions */
 #ifndef DLMALLOC_VERSION
@@ -534,6 +535,9 @@ MAX_RELEASE_CHECK_RATE   default: 4095 u
 #ifdef _WIN32
 #define WIN32 1
 #endif  /* _WIN32 */
+#ifdef _WIN64
+#define WIN32 1
+#endif /* _WIN64 */
 #ifdef _WIN32_WCE
 #define LACKS_FCNTL_H
 #define WIN32 1
@@ -543,7 +547,9 @@ MAX_RELEASE_CHECK_RATE   default: 4095 u
 #define WIN32_LEAN_AND_MEAN
 #include <windows.h>
 #include <tchar.h>
+#if !defined(HAVE_MMAP)
 #define HAVE_MMAP 1
+#endif
 #define HAVE_MORECORE 0
 #define LACKS_UNISTD_H
 #define LACKS_SYS_PARAM_H
@@ -1280,7 +1286,7 @@ typedef void* mspace;
   compiling with a different DEFAULT_GRANULARITY or dynamically
   setting with mallopt(M_GRANULARITY, value).
 */
-DLMALLOC_EXPORT mspace create_mspace(size_t capacity, int locked);
+DLMALLOC_EXPORT mspace create_mspace(size_t capacity, int locked, char* name);
 
 /*
   destroy_mspace destroys the given space, and attempts to return all
@@ -1299,7 +1305,7 @@ DLMALLOC_EXPORT size_t destroy_mspace(ms
   Destroying this space will deallocate all additionally allocated
   space (if possible) but not the initial base.
 */
-DLMALLOC_EXPORT mspace create_mspace_with_base(void* base, size_t capacity, int locked);
+DLMALLOC_EXPORT mspace create_mspace_with_base(void* base, size_t capacity, int locked, char* name);
 
 /*
   mspace_track_large_chunks controls whether requests for large chunks
@@ -1411,6 +1417,22 @@ DLMALLOC_EXPORT int mspace_trim(mspace m
 */
 DLMALLOC_EXPORT int mspace_mallopt(int, int);
 
+#if defined(USE_TEST_HOOK) && (USE_TEST_HOOK == 1)
+#if WIN32
+#define HOOKEXPORT __declspec(dllexport)
+#else
+#define HOOKEXPORT
+#endif
+
+#define DECLARE_HOOK(name) HOOKEXPORT void (*name ## _hook)(void)
+#define INSERT_HOOK(name) do { if (name ## _hook != NULL) { name ## _hook(); } } while (0)
+#else /* defined(USE_TEST_HOOK) && (USE_TEST_HOOK == 1) */
+#define DECLARE_HOOK(name) 
+#define INSERT_HOOK(name) do { } while (0)
+#endif /* defined(USE_TEST_HOOK) && (USE_TEST_HOOK == 1) */
+
+DECLARE_HOOK(mspace_malloc_in_lock);
+
 #endif /* MSPACES */
 
 #ifdef __cplusplus
@@ -1884,10 +1906,10 @@ static int spin_acquire_lock(int *sl) {
 }
 
 #define MLOCK_T               int
-#define TRY_LOCK(sl)          !CAS_LOCK(sl)
-#define RELEASE_LOCK(sl)      CLEAR_LOCK(sl)
-#define ACQUIRE_LOCK(sl)      (CAS_LOCK(sl)? spin_acquire_lock(sl) : 0)
-#define INITIAL_LOCK(sl)      (*sl = 0)
+#define TRY_LOCK(sl)          !CAS_LOCK(((mstate_ext)sl)->msp)
+#define RELEASE_LOCK(sl)      CLEAR_LOCK(((mstate_ext)sl)->msp)
+#define ACQUIRE_LOCK(sl)      (CAS_LOCK(((mstate_ext)sl)->msp)? spin_acquire_lock(((mstate_ext)sl)->msp) : 0)
+#define INITIAL_LOCK(sl)      (*(((mstate_ext)sl)->msp) = 0)
 #define DESTROY_LOCK(sl)      (0)
 static MLOCK_T malloc_global_mutex = 0;
 
@@ -1969,6 +1991,114 @@ static FORCEINLINE int recursive_try_loc
 #endif /* USE_RECURSIVE_LOCKS */
 
 #elif defined(WIN32) /* Win32 critical sections */
+#if defined(USE_PSHARED) && USE_PSHARED != 0
+#define MLOCK_T               HANDLE
+#define PREFIX_MUTEX_NAME "Global\\"
+#define SUFFIX_MUTEX_NAME " Mutex"
+static char* GetMutexName(char* name) {
+  int i;
+  size_t prefix_len;
+  size_t suffix_len;
+  size_t body_len;
+  size_t len;
+  char *p = NULL;
+  char *p_char = NULL;
+  char slash = '/';
+  
+  prefix_len = strlen(PREFIX_MUTEX_NAME);
+  suffix_len = strlen(SUFFIX_MUTEX_NAME);
+  body_len = strlen(name);
+  len = prefix_len + body_len + suffix_len + 1;
+  
+  p = malloc(len);
+  if (p != NULL) {
+    memcpy(&p[0], PREFIX_MUTEX_NAME, prefix_len);
+    memcpy(&p[prefix_len + body_len], SUFFIX_MUTEX_NAME, suffix_len);
+    p[prefix_len + body_len + suffix_len] = '\0';
+
+    for (i = 0; name[i] != '\0'; i++) {
+      if (name[i] == '\\') {
+        p_char = &slash;
+      } else {
+        p_char = &name[i];
+      }
+      p[prefix_len + i] = *p_char;
+    }
+  }
+
+  return p;
+}
+inline int AcquireLock(HANDLE mutex, int *ownerdead) {
+  DWORD ret;
+
+  if (ownerdead != NULL && *ownerdead != 0)
+    return WAIT_ABANDONED;
+
+  ret = WaitForSingleObject(mutex, INFINITE);
+  if (ret != WAIT_OBJECT_0) {
+    if (ret == WAIT_ABANDONED) {
+      ReleaseMutex(mutex);
+    }
+    if (ownerdead != NULL) {
+      *ownerdead = 1;
+    }
+  }
+  return ret;
+}
+inline int ReleaseLock(HANDLE mutex) {
+  if (ReleaseMutex(mutex) != 0)
+    return 0;
+  return 1;
+}
+inline int InitializeLock(HANDLE *pMutex, char *name, int open_only) {
+  char *mutex_name = NULL;
+  SECURITY_DESCRIPTOR secDesc;
+  SECURITY_ATTRIBUTES secAttr;
+  int ret = 0;
+
+  assert(pMutex != NULL);
+
+  InitializeSecurityDescriptor(&secDesc,SECURITY_DESCRIPTOR_REVISION);
+  SetSecurityDescriptorDacl(&secDesc, TRUE, 0, FALSE);
+  secAttr.nLength = sizeof(SECURITY_ATTRIBUTES);
+  secAttr.lpSecurityDescriptor = &secDesc;
+  secAttr.bInheritHandle = TRUE;
+  
+  *pMutex = NULL;
+  
+  mutex_name = GetMutexName(name);
+  if (mutex_name) {
+    DWORD err;
+    *pMutex = CreateMutexA(&secAttr, FALSE, mutex_name);
+    err = GetLastError();
+    assert(open_only != 0 || (open_only == 0 && err != ERROR_ALREADY_EXISTS));
+
+    if (open_only != 0 && err != ERROR_ALREADY_EXISTS) {
+      *pMutex = NULL;
+    }
+    free(mutex_name);
+  }
+
+  if (*pMutex == NULL) {
+    ret = 1;
+  }
+  
+  return ret;
+}
+
+inline int DestroyLock(HANDLE mutex) {
+  CloseHandle(mutex);
+  return 0;
+}
+#define ACQUIRE_LOCK(lk)      (AcquireLock((lk)->mutex, &((lk)->msp->ownerdead)))
+#define RELEASE_LOCK(lk)      (ReleaseLock((lk)->mutex))
+#define INITIAL_LOCK(lk)      (InitializeLock(&((lk)->mutex), (lk)->name, 0))
+#define DESTROY_LOCK(lk)      (DestroyLock((lk)->mutex))
+
+#define ACQUIRE_MALLOC_GLOBAL_LOCK() do {} while (0)
+#define RELEASE_MALLOC_GLOBAL_LOCK() do {} while (0)
+
+#else /* defined(USE_PSHARED) && USE_PSHARED != 0 */
 #define MLOCK_T               CRITICAL_SECTION
 #define ACQUIRE_LOCK(lk)      (EnterCriticalSection(lk), 0)
 #define RELEASE_LOCK(lk)      LeaveCriticalSection(lk)
@@ -1997,13 +2127,37 @@ static void init_malloc_global_mutex() {
   }
 }
 
+#endif /* defined(USE_PSHARED) && USE_PSHARED != 0 */
 #else /* pthreads-based locks */
+#if defined(USE_PSHARED) && USE_PSHARED != 0
+static int pthread_mutex_lock_keepdead(pthread_mutex_t *mutex, int *ownerdead) {
+  int ret;
+
+  if (ownerdead != NULL && *ownerdead != 0)
+    return EOWNERDEAD;
+
+  ret = pthread_mutex_lock(mutex);
+  if (ret != 0) {
+    if (ret == EOWNERDEAD) {
+      pthread_mutex_unlock(mutex);
+    }
+    if (ownerdead != NULL) {
+      *ownerdead = 1;
+    }
+  }
+  return ret;
+}
+#endif
 #define MLOCK_T               pthread_mutex_t
-#define ACQUIRE_LOCK(lk)      pthread_mutex_lock(lk)
-#define RELEASE_LOCK(lk)      pthread_mutex_unlock(lk)
-#define TRY_LOCK(lk)          (!pthread_mutex_trylock(lk))
-#define INITIAL_LOCK(lk)      pthread_init_lock(lk)
-#define DESTROY_LOCK(lk)      pthread_mutex_destroy(lk)
+#if defined(USE_PSHARED) && USE_PSHARED != 0
+#define ACQUIRE_LOCK(lk)      pthread_mutex_lock_keepdead(&((lk)->msp->mutex), &((lk)->msp->ownerdead))
+#else /* defined(USE_PSHARED) && USE_PSHARED != 0 */
+#define ACQUIRE_LOCK(lk)      pthread_mutex_lock(&((lk)->msp->mutex))
+#endif /* defined(USE_PSHARED) && USE_PSHARED != 0 */
+#define RELEASE_LOCK(lk)      pthread_mutex_unlock(&((lk)->msp->mutex))
+#define TRY_LOCK(lk)          (!pthread_mutex_trylock(&((lk)->msp->mutex)))
+#define INITIAL_LOCK(lk)      pthread_init_lock(&((lk)->msp->mutex))
+#define DESTROY_LOCK(lk)      pthread_mutex_destroy(&((lk)->msp->mutex))
 
 #if defined(USE_RECURSIVE_LOCKS) && USE_RECURSIVE_LOCKS != 0 && defined(linux) && !defined(PTHREAD_MUTEX_RECURSIVE)
 /* Cope with old-style linux recursive lock initialization by adding */
@@ -2014,7 +2168,9 @@ extern int pthread_mutexattr_setkind_np
 #define pthread_mutexattr_settype(x,y) pthread_mutexattr_setkind_np(x,y)
 #endif /* USE_RECURSIVE_LOCKS ... */
 
+#if !defined(USE_PSHARED) || USE_PSHARED == 0
 static MLOCK_T malloc_global_mutex = PTHREAD_MUTEX_INITIALIZER;
+#endif
 
 static int pthread_init_lock (MLOCK_T *lk) {
   pthread_mutexattr_t attr;
@@ -2022,11 +2178,20 @@ static int pthread_init_lock (MLOCK_T *l
 #if defined(USE_RECURSIVE_LOCKS) && USE_RECURSIVE_LOCKS != 0
   if (pthread_mutexattr_settype(&attr, PTHREAD_MUTEX_RECURSIVE)) return 1;
 #endif
+#if defined(USE_PSHARED) && USE_PSHARED != 0
+  if (pthread_mutexattr_setpshared(&attr, PTHREAD_PROCESS_SHARED)) return 1;
+  if (pthread_mutexattr_setrobust(&attr, PTHREAD_MUTEX_ROBUST)) return 1;
+#endif
   if (pthread_mutex_init(lk, &attr)) return 1;
   if (pthread_mutexattr_destroy(&attr)) return 1;
   return 0;
 }
 
+#if defined(USE_PSHARED) && USE_PSHARED != 0
+#define ACQUIRE_MALLOC_GLOBAL_LOCK() do {} while (0)
+#define RELEASE_MALLOC_GLOBAL_LOCK() do {} while (0)
+#endif
+
 #endif /* ... lock types ... */
 
 /* Common code for all lock types */
@@ -2597,10 +2762,19 @@ struct malloc_state {
   msegment   seg;
   void*      extp;      /* Unused but available for extensions */
   size_t     exts;
+  int        ownerdead;
 };
 
 typedef struct malloc_state*    mstate;
 
+struct malloc_state_ext {
+  mstate     msp;
+  MLOCK_T    mutex;
+  char*      name;
+};
+
+typedef struct malloc_state_ext* mstate_ext;
+
 /* ------------- Global malloc_state and malloc_params ------------------- */
 
 /*
@@ -2738,8 +2912,8 @@ static int has_segment_link(mstate m, ms
 */
 
 #if USE_LOCKS
-#define PREACTION(M)  ((use_lock(M))? ACQUIRE_LOCK(&(M)->mutex) : 0)
-#define POSTACTION(M) { if (use_lock(M)) RELEASE_LOCK(&(M)->mutex); }
+#define PREACTION(M)  ((use_lock(((M)->msp)))? ACQUIRE_LOCK(M) : 0)
+#define POSTACTION(M) { if (use_lock(((M)->msp))) RELEASE_LOCK(M); }
 #else /* USE_LOCKS */
 
 #ifndef PREACTION
@@ -3479,10 +3653,11 @@ static void do_check_malloc_state(mstate
 /* ----------------------------- statistics ------------------------------ */
 
 #if !NO_MALLINFO
-static struct mallinfo internal_mallinfo(mstate m) {
+static struct mallinfo internal_mallinfo(mstate_ext me) {
   struct mallinfo nm = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
   ensure_initialization();
-  if (!PREACTION(m)) {
+  mstate m = me->msp;
+  if (!PREACTION(me)) {
     check_malloc_state(m);
     if (is_initialized(m)) {
       size_t nfree = SIZE_T_ONE; /* top always free */
@@ -3513,16 +3688,17 @@ static struct mallinfo internal_mallinfo
       nm.keepcost = m->topsize;
     }
 
-    POSTACTION(m);
+    POSTACTION(me);
   }
   return nm;
 }
 #endif /* !NO_MALLINFO */
 
 #if !NO_MALLOC_STATS
-static void internal_malloc_stats(mstate m) {
+static void internal_malloc_stats(mstate_ext me) {
   ensure_initialization();
-  if (!PREACTION(m)) {
+  mstate m = me->msp;
+  if (!PREACTION(me)) {
     size_t maxfp = 0;
     size_t fp = 0;
     size_t used = 0;
@@ -3544,7 +3720,7 @@ static void internal_malloc_stats(mstate
         s = s->next;
       }
     }
-    POSTACTION(m); /* drop lock */
+    POSTACTION(me); /* drop lock */
     fprintf(stderr, "max system bytes = %10lu\n", (unsigned long)(maxfp));
     fprintf(stderr, "system bytes     = %10lu\n", (unsigned long)(fp));
     fprintf(stderr, "in use bytes     = %10lu\n", (unsigned long)(used));
@@ -4882,8 +5058,9 @@ static mchunkptr try_realloc_chunk(mstat
   return newp;
 }
 
-static void* internal_memalign(mstate m, size_t alignment, size_t bytes) {
+static void* internal_memalign(mstate_ext me, size_t alignment, size_t bytes) {
   void* mem = 0;
+  mstate m = me->msp;
   if (alignment <  MIN_CHUNK_SIZE) /* must be at least a minimum chunk size */
     alignment = MIN_CHUNK_SIZE;
   if ((alignment & (alignment-SIZE_T_ONE)) != 0) {/* Ensure a power of 2 */
@@ -4902,7 +5079,7 @@ static void* internal_memalign(mstate m,
     mem = internal_malloc(m, req);
     if (mem != 0) {
       mchunkptr p = mem2chunk(mem);
-      if (PREACTION(m))
+      if (PREACTION(me))
         return 0;
       if ((((size_t)(mem)) & (alignment - 1)) != 0) { /* misaligned */
         /*
@@ -4950,7 +5127,7 @@ static void* internal_memalign(mstate m,
       assert (chunksize(p) >= nb);
       assert(((size_t)mem & (alignment - 1)) == 0);
       check_inuse_chunk(m, p);
-      POSTACTION(m);
+      POSTACTION(me);
     }
   }
   return mem;
@@ -4963,7 +5140,7 @@ static void* internal_memalign(mstate m,
     bit 0 set if all elements are same size (using sizes[0])
     bit 1 set if elements should be zeroed
 */
-static void** ialloc(mstate m,
+static void** ialloc(mstate_ext me,
                      size_t n_elements,
                      size_t* sizes,
                      int opts,
@@ -4980,6 +5157,7 @@ static void** ialloc(mstate m,
   flag_t    was_enabled;    /* to disable mmap */
   size_t    size;
   size_t    i;
+  mstate    m = me->msp;
 
   ensure_initialization();
   /* compute array length, if needed */
@@ -4992,7 +5170,7 @@ static void** ialloc(mstate m,
   else {
     /* if empty req, must still return chunk representing empty array */
     if (n_elements == 0)
-      return (void**)internal_malloc(m, 0);
+      return (void**)internal_malloc(me, 0);
     marray = 0;
     array_size = request2size(n_elements * (sizeof(void*)));
   }
@@ -5024,7 +5202,7 @@ static void** ialloc(mstate m,
   if (mem == 0)
     return 0;
 
-  if (PREACTION(m)) return 0;
+  if (PREACTION(me)) return 0;
   p = mem2chunk(mem);
   remainder_size = chunksize(p);
 
@@ -5078,7 +5256,7 @@ static void** ialloc(mstate m,
 
 #endif /* DEBUG */
 
-  POSTACTION(m);
+  POSTACTION(me);
   return marray;
 }
 
@@ -5089,9 +5267,10 @@ static void** ialloc(mstate m,
    chunks before freeing, which will occur often if allocated
    with ialloc or the array is sorted.
 */
-static size_t internal_bulk_free(mstate m, void* array[], size_t nelem) {
+static size_t internal_bulk_free(mstate_ext me, void* array[], size_t nelem) {
   size_t unfreed = 0;
-  if (!PREACTION(m)) {
+  mstate m = me->msp;
+  if (!PREACTION(me)) {
     void** a;
     void** fence = &(array[nelem]);
     for (a = array; a != fence; ++a) {
@@ -5126,7 +5305,7 @@ static size_t internal_bulk_free(mstate
     }
     if (should_trim(m, m->topsize))
       sys_trim(m, 0);
-    POSTACTION(m);
+    POSTACTION(me);
   }
   return unfreed;
 }
@@ -5319,7 +5498,7 @@ void dlmalloc_inspect_all(void(*handler)
                                          void* callback_arg),
                           void* arg) {
   ensure_initialization();
-  if (!PREACTION(gm)) {
+  if (!PREACTION(gm))) {
     internal_inspect_all(gm, handler, arg);
     POSTACTION(gm);
   }
@@ -5391,13 +5570,22 @@ size_t dlmalloc_usable_size(void* mem) {
 
 #if MSPACES
 
-static mstate init_user_mstate(char* tbase, size_t tsize) {
+static mstate_ext init_user_mstate(char* tbase, size_t tsize, char* name) {
   size_t msize = pad_request(sizeof(struct malloc_state));
   mchunkptr mn;
   mchunkptr msp = align_as_chunk(tbase);
   mstate m = (mstate)(chunk2mem(msp));
+  mstate_ext me = (mstate_ext)malloc(sizeof(struct malloc_state_ext));
+  if (me == NULL)
+    return (mstate_ext)me;
   memset(m, 0, msize);
-  (void)INITIAL_LOCK(&m->mutex);
+  me->msp = m;
+  me->name = name;
+  (void)name;
+  if (INITIAL_LOCK(me)) {
+    free(me);
+    return (mstate_ext)0;
+  }
   msp->head = (msize|INUSE_BITS);
   m->seg.base = m->least_addr = tbase;
   m->seg.size = m->footprint = m->max_footprint = tsize;
@@ -5406,16 +5594,18 @@ static mstate init_user_mstate(char* tba
   m->mflags = mparams.default_mflags;
   m->extp = 0;
   m->exts = 0;
+  m->ownerdead = 0;
   disable_contiguous(m);
   init_bins(m);
   mn = next_chunk(mem2chunk(m));
   init_top(m, mn, (size_t)((tbase + tsize) - (char*)mn) - TOP_FOOT_SIZE);
   check_top_chunk(m, m->top);
-  return m;
+  return me;
 }
 
-mspace create_mspace(size_t capacity, int locked) {
+mspace create_mspace(size_t capacity, int locked, char* name) {
   mstate m = 0;
+  mstate_ext mext = 0;
   size_t msize;
   ensure_initialization();
   msize = pad_request(sizeof(struct malloc_state));
@@ -5425,32 +5615,62 @@ mspace create_mspace(size_t capacity, in
     size_t tsize = granularity_align(rs);
     char* tbase = (char*)(CALL_MMAP(tsize));
     if (tbase != CMFAIL) {
-      m = init_user_mstate(tbase, tsize);
-      m->seg.sflags = USE_MMAP_BIT;
-      set_lock(m, locked);
+      mext = init_user_mstate(tbase, tsize, name);
+      if (mext != NULL) {
+        m = mext->msp;
+        m->seg.sflags = USE_MMAP_BIT;
+        set_lock(m, locked);
+      }
     }
   }
-  return (mspace)m;
+  return (mspace)mext;
 }
 
-mspace create_mspace_with_base(void* base, size_t capacity, int locked) {
+mspace create_mspace_with_base(void* base, size_t capacity, int locked, char* name) {
   mstate m = 0;
+  mstate_ext mext = 0;
   size_t msize;
+  (void)name;
   ensure_initialization();
   msize = pad_request(sizeof(struct malloc_state));
   if (capacity > msize + TOP_FOOT_SIZE &&
       capacity < (size_t) -(msize + TOP_FOOT_SIZE + mparams.page_size)) {
-    m = init_user_mstate((char*)base, capacity);
-    m->seg.sflags = EXTERN_BIT;
-    set_lock(m, locked);
+    mext = init_user_mstate((char*)base, capacity, name);
+    if (mext != NULL) {
+      m = mext->msp;
+      m->seg.sflags = EXTERN_BIT;
+      set_lock(m, locked);
+    }
+  }
+  return (mspace)mext;
+}
+
+mspace attach_mspace(void *base, char* name) {
+  mstate m = 0;
+  mstate_ext mext = 0;
+  mchunkptr msp = NULL;
+  ensure_initialization();
+  mext = (mstate_ext)malloc(sizeof(struct malloc_state_ext));
+  if (mext == NULL)
+    return (mspace)0;
+  msp = align_as_chunk((char*)base);
+  m = (mstate)(chunk2mem(msp));
+  mext->msp = m;
+  mext->name = name;
+#if defined(WIN32) && (defined(USE_PSHARED) && USE_PSHARED != 0)
+  if (InitializeLock(&(mext->mutex), mext->name, 1)) {
+    free(mext);
+    mext = 0;
   }
-  return (mspace)m;
+#endif
+  return (mspace)mext;
 }
 
 int mspace_track_large_chunks(mspace msp, int enable) {
   int ret = 0;
-  mstate ms = (mstate)msp;
-  if (!PREACTION(ms)) {
+  mstate_ext me = (mstate_ext)msp;
+  mstate ms = me->msp;
+  if (!PREACTION(me)) {
     if (!use_mmap(ms)) {
       ret = 1;
     }
@@ -5459,17 +5679,30 @@ int mspace_track_large_chunks(mspace msp
     } else {
       disable_mmap(ms);
     }
-    POSTACTION(ms);
+    POSTACTION(me);
   }
   return ret;
 }
 
+void detach_mspace(mspace msp) {
+  mstate_ext me = (mstate_ext)msp;
+  mstate ms = me->msp;
+  (void)ms;
+  if (ok_magic(ms)) {
+    free(msp);
+  }
+  else {
+    USAGE_ERROR_ACTION(ms,ms);
+  }
+}
+
 size_t destroy_mspace(mspace msp) {
   size_t freed = 0;
-  mstate ms = (mstate)msp;
+  mstate_ext me = (mstate_ext)msp;
+  mstate ms = me->msp;
   if (ok_magic(ms)) {
     msegmentptr sp = &ms->seg;
-    (void)DESTROY_LOCK(&ms->mutex); /* destroy before unmapped */
+    (void)DESTROY_LOCK(me); /* destroy before unmapped */
     while (sp != 0) {
       char* base = sp->base;
       size_t size = sp->size;
@@ -5480,6 +5713,7 @@ size_t destroy_mspace(mspace msp) {
           CALL_MUNMAP(base, size) == 0)
         freed += size;
     }
+    free(msp);
   }
   else {
     USAGE_ERROR_ACTION(ms,ms);
@@ -5493,14 +5727,16 @@ size_t destroy_mspace(mspace msp) {
 */
 
 void* mspace_malloc(mspace msp, size_t bytes) {
-  mstate ms = (mstate)msp;
+  mstate_ext me = (mstate_ext)msp;
+  mstate ms = me->msp;
   if (!ok_magic(ms)) {
     USAGE_ERROR_ACTION(ms,ms);
     return 0;
   }
-  if (!PREACTION(ms)) {
+  if (!PREACTION(me)) {
     void* mem;
     size_t nb;
+    INSERT_HOOK(mspace_malloc_in_lock);
     if (bytes <= MAX_SMALL_REQUEST) {
       bindex_t idx;
       binmap_t smallbits;
@@ -5599,7 +5835,7 @@ void* mspace_malloc(mspace msp, size_t b
     mem = sys_alloc(ms, nb);
 
   postaction:
-    POSTACTION(ms);
+    POSTACTION(me);
     return mem;
   }
 
@@ -5613,13 +5849,14 @@ void mspace_free(mspace msp, void* mem)
     mstate fm = get_mstate_for(p);
     (void)msp; /* placate people compiling -Wunused */
 #else /* FOOTERS */
-    mstate fm = (mstate)msp;
+    mstate_ext me = (mstate_ext)msp;
+    mstate fm = me->msp;
 #endif /* FOOTERS */
     if (!ok_magic(fm)) {
       USAGE_ERROR_ACTION(fm, p);
       return;
     }
-    if (!PREACTION(fm)) {
+    if (!PREACTION(me)) {
       check_inuse_chunk(fm, p);
       if (RTCHECK(ok_address(fm, p) && ok_inuse(p))) {
         size_t psize = chunksize(p);
@@ -5702,7 +5939,7 @@ void mspace_free(mspace msp, void* mem)
     erroraction:
       USAGE_ERROR_ACTION(fm, p);
     postaction:
-      POSTACTION(fm);
+      POSTACTION(me);
     }
   }
 }
@@ -5710,7 +5947,9 @@ void mspace_free(mspace msp, void* mem)
 void* mspace_calloc(mspace msp, size_t n_elements, size_t elem_size) {
   void* mem;
   size_t req = 0;
-  mstate ms = (mstate)msp;
+  mstate_ext me = (mstate_ext)msp;
+  mstate ms = me->msp;
+  (void)ms;
   if (!ok_magic(ms)) {
     USAGE_ERROR_ACTION(ms,ms);
     return 0;
@@ -5721,7 +5960,7 @@ void* mspace_calloc(mspace msp, size_t n
         (req / n_elements != elem_size))
       req = MAX_SIZE_T; /* force downstream failure on overflow */
   }
-  mem = internal_malloc(ms, req);
+  mem = internal_malloc(me, req);
   if (mem != 0 && calloc_must_clear(mem2chunk(mem)))
     memset(mem, 0, req);
   return mem;
@@ -5744,7 +5983,8 @@ void* mspace_realloc(mspace msp, void* o
     size_t nb = request2size(bytes);
     mchunkptr oldp = mem2chunk(oldmem);
 #if ! FOOTERS
-    mstate m = (mstate)msp;
+    mstate_ext me = (mstate_ext)msp;
+    mstate m = me->msp;
 #else /* FOOTERS */
     mstate m = get_mstate_for(oldp);
     if (!ok_magic(m)) {
@@ -5752,9 +5992,9 @@ void* mspace_realloc(mspace msp, void* o
       return 0;
     }
 #endif /* FOOTERS */
-    if (!PREACTION(m)) {
+    if (!PREACTION(me)) {
       mchunkptr newp = try_realloc_chunk(m, oldp, nb, 1);
-      POSTACTION(m);
+      POSTACTION(me);
       if (newp != 0) {
         check_inuse_chunk(m, newp);
         mem = chunk2mem(newp);
@@ -5782,7 +6022,8 @@ void* mspace_realloc_in_place(mspace msp
       size_t nb = request2size(bytes);
       mchunkptr oldp = mem2chunk(oldmem);
 #if ! FOOTERS
-      mstate m = (mstate)msp;
+      mstate_ext me = (mstate_ext)msp;
+      mstate m = me->msp;
 #else /* FOOTERS */
       mstate m = get_mstate_for(oldp);
       (void)msp; /* placate people compiling -Wunused */
@@ -5791,9 +6032,9 @@ void* mspace_realloc_in_place(mspace msp
         return 0;
       }
 #endif /* FOOTERS */
-      if (!PREACTION(m)) {
+      if (!PREACTION(me)) {
         mchunkptr newp = try_realloc_chunk(m, oldp, nb, 0);
-        POSTACTION(m);
+        POSTACTION(me);
         if (newp == oldp) {
           check_inuse_chunk(m, newp);
           mem = oldmem;
@@ -5805,39 +6046,44 @@ void* mspace_realloc_in_place(mspace msp
 }
 
 void* mspace_memalign(mspace msp, size_t alignment, size_t bytes) {
-  mstate ms = (mstate)msp;
+  mstate ms = ((mstate_ext)msp)->msp;
+  (void)ms;
   if (!ok_magic(ms)) {
     USAGE_ERROR_ACTION(ms,ms);
     return 0;
   }
   if (alignment <= MALLOC_ALIGNMENT)
     return mspace_malloc(msp, bytes);
-  return internal_memalign(ms, alignment, bytes);
+  return internal_memalign(msp, alignment, bytes);
 }
 
 void** mspace_independent_calloc(mspace msp, size_t n_elements,
                                  size_t elem_size, void* chunks[]) {
   size_t sz = elem_size; /* serves as 1-element array */
-  mstate ms = (mstate)msp;
+  mstate_ext me = (mstate_ext)msp;
+  mstate ms = me->msp;
+  (void)ms;
   if (!ok_magic(ms)) {
     USAGE_ERROR_ACTION(ms,ms);
     return 0;
   }
-  return ialloc(ms, n_elements, &sz, 3, chunks);
+  return ialloc(me, n_elements, &sz, 3, chunks);
 }
 
 void** mspace_independent_comalloc(mspace msp, size_t n_elements,
                                    size_t sizes[], void* chunks[]) {
-  mstate ms = (mstate)msp;
+  mstate_ext me = (mstate_ext)msp;
+  mstate ms = me->msp;
+  (void)ms;
   if (!ok_magic(ms)) {
     USAGE_ERROR_ACTION(ms,ms);
     return 0;
   }
-  return ialloc(ms, n_elements, sizes, 0, chunks);
+  return ialloc(me, n_elements, sizes, 0, chunks);
 }
 
 size_t mspace_bulk_free(mspace msp, void* array[], size_t nelem) {
-  return internal_bulk_free((mstate)msp, array, nelem);
+  return internal_bulk_free((mstate_ext)msp, array, nelem);
 }
 
 #if MALLOC_INSPECT_ALL
@@ -5847,11 +6093,12 @@ void mspace_inspect_all(mspace msp,
                                        size_t used_bytes,
                                        void* callback_arg),
                         void* arg) {
-  mstate ms = (mstate)msp;
+  mstate_ext me = (mstate_ext)msp;
+  mstate ms = me->msp;
   if (ok_magic(ms)) {
-    if (!PREACTION(ms)) {
+    if (!PREACTION(me)) {
       internal_inspect_all(ms, handler, arg);
-      POSTACTION(ms);
+      POSTACTION(me);
     }
   }
   else {
@@ -5862,11 +6109,12 @@ void mspace_inspect_all(mspace msp,
 
 int mspace_trim(mspace msp, size_t pad) {
   int result = 0;
-  mstate ms = (mstate)msp;
+  mstate_ext me = (mstate_ext)msp;
+  mstate ms = me->msp;
   if (ok_magic(ms)) {
-    if (!PREACTION(ms)) {
+    if (!PREACTION(me)) {
       result = sys_trim(ms, pad);
-      POSTACTION(ms);
+      POSTACTION(me);
     }
   }
   else {
@@ -5877,9 +6125,11 @@ int mspace_trim(mspace msp, size_t pad)
 
 #if !NO_MALLOC_STATS
 void mspace_malloc_stats(mspace msp) {
-  mstate ms = (mstate)msp;
+  mstate_ext me = (mstate_ext)msp;
+  mstate ms = me->msp;
+  (void)ms;
   if (ok_magic(ms)) {
-    internal_malloc_stats(ms);
+    internal_malloc_stats(me);
   }
   else {
     USAGE_ERROR_ACTION(ms,ms);
@@ -5889,7 +6139,7 @@ void mspace_malloc_stats(mspace msp) {
 
 size_t mspace_footprint(mspace msp) {
   size_t result = 0;
-  mstate ms = (mstate)msp;
+  mstate ms = ((mstate_ext)msp)->msp;
   if (ok_magic(ms)) {
     result = ms->footprint;
   }
@@ -5901,7 +6151,7 @@ size_t mspace_footprint(mspace msp) {
 
 size_t mspace_max_footprint(mspace msp) {
   size_t result = 0;
-  mstate ms = (mstate)msp;
+  mstate ms = ((mstate_ext)msp)->msp;
   if (ok_magic(ms)) {
     result = ms->max_footprint;
   }
@@ -5913,7 +6163,7 @@ size_t mspace_max_footprint(mspace msp)
 
 size_t mspace_footprint_limit(mspace msp) {
   size_t result = 0;
-  mstate ms = (mstate)msp;
+  mstate ms = ((mstate_ext)msp)->msp;
   if (ok_magic(ms)) {
     size_t maf = ms->footprint_limit;
     result = (maf == 0) ? MAX_SIZE_T : maf;
@@ -5926,7 +6176,7 @@ size_t mspace_footprint_limit(mspace msp
 
 size_t mspace_set_footprint_limit(mspace msp, size_t bytes) {
   size_t result = 0;
-  mstate ms = (mstate)msp;
+  mstate ms = ((mstate_ext)msp)->msp;
   if (ok_magic(ms)) {
     if (bytes == 0)
       result = granularity_align(1); /* Use minimal size */
@@ -5944,11 +6194,13 @@ size_t mspace_set_footprint_limit(mspace
 
 #if !NO_MALLINFO
 struct mallinfo mspace_mallinfo(mspace msp) {
-  mstate ms = (mstate)msp;
+  mstate_ext me = (mstate_ext)msp;
+  mstate ms = me->msp;
+  (void)ms;
   if (!ok_magic(ms)) {
     USAGE_ERROR_ACTION(ms,ms);
   }
-  return internal_mallinfo(ms);
+  return internal_mallinfo(me);
 }
 #endif /* NO_MALLINFO */
 
@@ -5965,6 +6217,14 @@ int mspace_mallopt(int param_number, int
   return change_mparam(param_number, value);
 }
 
+int mspace_get_ownerdead(mspace msp) {
+  mstate ms = ((mstate_ext)msp)->msp;
+  if (!ok_magic(ms)) {
+    USAGE_ERROR_ACTION(ms,ms);
+  }
+  return ms->ownerdead;
+}
+
 #endif /* MSPACES */
 
 
--- dlmalloc/malloc.h	2020-02-19 17:18:59.688398842 +0900
+++ psm_dlmalloc.h	2020-07-01 08:22:20.348463407 +0900
@@ -29,6 +29,14 @@ extern "C" {
 
 #include <stddef.h>   /* for size_t */
 
+/* added defines */
+#define ONLY_MSPACES 1
+#define MSPACES 1
+#define USE_LOCKS 1
+#define USE_SPIN_LOCKS 0
+#define HAVE_MMAP 0
+#define USE_PSHARED 1
+
 #ifndef ONLY_MSPACES
 #define ONLY_MSPACES 0     /* define to a value */
 #elif ONLY_MSPACES != 0
@@ -539,7 +547,7 @@ typedef void* mspace;
   compiling with a different DEFAULT_GRANULARITY or dynamically
   setting with mallopt(M_GRANULARITY, value).
 */
-mspace create_mspace(size_t capacity, int locked);
+mspace create_mspace(size_t capacity, int locked, char* name);
 
 /*
   destroy_mspace destroys the given space, and attempts to return all
@@ -558,7 +566,17 @@ size_t destroy_mspace(mspace msp);
   Destroying this space will deallocate all additionally allocated
   space (if possible) but not the initial base.
 */
-mspace create_mspace_with_base(void* base, size_t capacity, int locked);
+mspace create_mspace_with_base(void* base, size_t capacity, int locked, char* name);
+
+/*
+  attach_mspace returns existing space which is mapped in the base address.
+*/
+mspace attach_mspace(void *base, char* name);
+
+/*
+  detach_mspace frees allocated resources in process local.
+*/
+void detach_mspace(mspace msp);
 
 /*
   mspace_track_large_chunks controls whether requests for large chunks
@@ -611,6 +629,7 @@ size_t mspace_set_footprint_limit(mspace
 void mspace_inspect_all(mspace msp, 
                         void(*handler)(void *, void *, size_t, void*),
                         void* arg);
+int mspace_get_ownerdead(mspace msp);
 #endif  /* MSPACES */
 
 #ifdef __cplusplus
